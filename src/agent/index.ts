import {
  Annotation,
  END,
  LangGraphRunnableConfig,
  MessagesAnnotation,
  START,
  StateGraph,
} from "@langchain/langgraph";
import { z } from "zod";
import { BaseMessage } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { ChatAnthropic } from "@langchain/anthropic";
import { DEFAULT_SYSTEM_RULES } from "../constants";
import { getRulesFromStore, putRulesInStore } from "./utils";

const DEFAULT_SYSTEM_RULES_STRING = `- ${DEFAULT_SYSTEM_RULES.join("\n- ")}`;
const DEFAULT_RULES_STRING = "*no rules have been set yet*";

const GraphAnnotation = Annotation.Root({
  ...MessagesAnnotation.spec,
  /**
   * Whether or not writing content was generated in the conversation.
   */
  contentGenerated: Annotation<boolean>,
});

const GraphConfig = Annotation.Root({
  /**
   * The system rules to always include when generating responses.
   * This is editable by the user.
   */
  systemRules: Annotation<string>,
  /**
   * Whether or not the user has accepted the text generated by the AI.
   * If this is true, the graph will route to a node which generates rules.
   */
  hasAcceptedText: Annotation<boolean>,
});

const RULES_PROMPT = `The user has defined two sets of rules. The first set is for style guidelines, and the second set is for content guidelines.

<style_rules>
{styleRules}
</style_rules>

<content_rules>
{contentRules}
</content_rules>`;

const SYSTEM_PROMPT = `You are a helpful assistant tasked with thoughtfully fulfilling the requests of the user.

System rules:

<system_rules>
{systemRules}
</system_rules>

{rulesPrompt}`;

const callModel = async (
  state: typeof GraphAnnotation.State,
  config: LangGraphRunnableConfig
) => {
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-20240620",
    temperature: 0,
  });

  const { styleRules, contentRules } = await getRulesFromStore(config);

  const styleRulesString = styleRules ? `- ${styleRules.join("\n - ")}` : null;
  const contentRulesString = contentRules
    ? `- ${contentRules.join("\n - ")}`
    : null;

  let systemPrompt = SYSTEM_PROMPT.replace(
    "{systemRules}",
    config?.configurable?.systemRules ?? DEFAULT_SYSTEM_RULES_STRING
  );
  if (styleRulesString || contentRulesString) {
    systemPrompt = systemPrompt
      .replace("{rulesPrompt}", RULES_PROMPT)
      .replace("{styleRules}", styleRulesString || DEFAULT_RULES_STRING)
      .replace("{contentRules}", contentRulesString || DEFAULT_RULES_STRING);
  } else {
    systemPrompt = systemPrompt.replace("{rulesPrompt}", "");
  }

  const response = await model.invoke(
    [
      {
        role: "system",
        content: systemPrompt,
      },
      ...state.messages,
    ],
    config
  );
  return { messages: [response] };
};

const _prepareConversation = (messages: BaseMessage[]): string => {
  return messages
    .map((msg, i) => {
      if (typeof msg.content !== "string") return "";
      return `<${msg._getType()}_message index={${i}}>\n${msg.content}\n</${msg._getType()}_message>`;
    })
    .join("\n\n");
};

/**
 * This node generates insights based on the changes, or followup messages
 * that have been made by the user. It does the following:
 * 1. Sets a system message describing the task, and existing user rules, and how messages in the history can be formatted. (e.g an AI Message, followed by a human message that is prefixed with "REVISED MESSAGE").
 * 2. Passes the entire history to the LLM
 * 3. Uses `withStructuredOutput` to generate structured rules based on conversation or revisions.
 * 4. Updates the `userRules` shared value with the new rules.
 *
 * The LLM will always re-generate the entire rules list, so it is important to pass the entire history to the model.
 * @param state The current state of the graph
 */
const generateInsights = async (
  state: typeof GraphAnnotation.State,
  config: LangGraphRunnableConfig
) => {
  const systemPrompt = `This conversation contains back and fourth between an AI assistant, and a user who is using the assistant to generate text.

User messages which are prefixed with "REVISED MESSAGE" contain the entire revised text the user made to the assistant message directly before in the conversation.
Revisions are made directly by users, so you should pay VERY close attention to every single change made, no matter how small. These should be heavily considered when generating rules.

Important aspects of revisions to consider:
- Deletions: What did the user remove? Do you need a rule to avoid adding this in the future?
- Tone: Did they change the overall tone? Do you need a rule to ensure this tone is maintained?
- Structure: Did they change the structure of the text? This is important to remember, as it may be a common pattern. 

There also may be additional back and fourth between the user and the assistant.

Based on the conversation, and paying particular attention to any changes made in the "REVISED MESSAGE", your job is to create a list of rules to use in the future to help the AI assistant better generate text.

These rules should be split into two categories:
1. Style guidelines: These rules should focus on the style, tone, and structure of the text.
2. Content guidelines: These rules should focus on the content, context, and purpose of the text. Think of this as the business logic or domain-specific rules.

In your response, include every single rule you want the AI assistant to follow in the future. You should list rules based on a combination of the existing conversation as well as previous rules.
You can modify previous rules if you think the new conversation has helpful information, or you can delete old rules if they don't seem relevant, or you can add new rules based on the conversation.

Refrain from adding overly generic rules like "follow instructions". These generic rules are already outlined in the "system_rules" below.
Instead, focus your attention on specific details, writing style, or other aspects of the conversation that you think are important for the AI to follow.

The user has defined the following rules:

<style_rules>
{styleRules}
</style_rules>

<content_rules>
{contentRules}
</content_rules>

Here is the conversation:

<conversation>
{conversation}
</conversation>

And here are the default system rules:

<system_rules>
{systemRules}
</system_rules>

Respond with updated rules to keep in mind for future conversations. Try to keep the rules you list high signal-to-noise - don't include unnecessary ones, but make sure the ones you do add are descriptive. Combine ones that seem similar and/or contradictory`;

  const { styleRules, contentRules } = await getRulesFromStore(config);

  const styleRulesString = styleRules
    ? `- ${styleRules.join("\n - ")}`
    : DEFAULT_RULES_STRING;
  const contentRulesString = contentRules
    ? `- ${contentRules.join("\n - ")}`
    : DEFAULT_RULES_STRING;

  const prompt = systemPrompt
    .replace(
      "{systemRules}",
      config.configurable?.systemRules ?? DEFAULT_SYSTEM_RULES_STRING
    )
    .replace("{styleRules}", styleRulesString)
    .replace("{contentRules}", contentRulesString)
    .replace("{conversation}", _prepareConversation(state.messages));

  const userRulesSchema = z.object({
    contentRules: z
      .array(z.string())
      .describe(
        "List of rules focusing on content, context, and purpose of the text"
      ),
    styleRules: z
      .array(z.string())
      .describe(
        "List of rules focusing on style, tone, and structure of the text"
      ),
  });

  const modelWithStructuredOutput = new ChatAnthropic({
    model: "claude-3-5-sonnet-20240620",
    temperature: 0,
  }).withStructuredOutput(userRulesSchema, { name: "userRules" });

  const result = await modelWithStructuredOutput.invoke(
    [
      {
        role: "user",
        content: prompt,
      },
    ],
    config
  );

  await putRulesInStore(config, result);

  return {
    userAcceptedText: false,
  };
};

const wasContentGenerated = async (state: typeof GraphAnnotation.State) => {
  const { messages } = state;

  const prompt = `The following is the ending of a conversation between a user and an AI assistant. The assistant has been tasked with generating writing content (e.g a blog post, or tweet) for the user.
Carefully read the conversation and determine whether or not writing content was generated by the assistant, in the most recent assistant message.

In some cases, the assistant is responding to a question and did not generate any content.
In other cases the assistant is generating content, or revising content.

If the assistant is discussing content, but no actual content was generated, or included in the response, set 'contentGenerated' to false.
If the assistant generated content, set 'contentGenerated' to true.

<conversation>
{conversation}
</conversation>`;
  const schema = z.object({
    contentGenerated: z
      .boolean()
      .describe(
        "Whether or not content (e.g a tweet, or blog post) was generated in the most recent assistant message."
      ),
  });

  const formattedPrompt = (() => {
    // Find the index of the last human message
    const lastHumanMessageIndex = messages.findLastIndex(
      (msg) => msg._getType() === "human"
    );

    // If a human message is found, slice the array from that index
    const slicedMessages =
      lastHumanMessageIndex !== -1
        ? messages.slice(lastHumanMessageIndex)
        : messages;

    // Prepare the conversation with the sliced messages
    return prompt.replace(
      "{conversation}",
      _prepareConversation(slicedMessages)
    );
  })();

  const model = new ChatAnthropic({
    model: "claude-3-haiku-20240307",
    temperature: 0,
  }).withStructuredOutput(schema, { name: "was_content_generated" });

  return model.invoke([
    {
      role: "user",
      content: formattedPrompt,
    },
  ]);
};

/**
 * Conditional edge which is always called first. This edge
 * determines whether or not revisions have been made, and if so,
 * generate insights to then set under user rules.
 * @param {typeof GraphAnnotation.State} state The current state of the graph
 */
const shouldGenerateInsights = (
  _state: typeof GraphAnnotation.State,
  config?: RunnableConfig
) => {
  const { hasAcceptedText = false } = config?.configurable ?? {};

  if (hasAcceptedText) {
    return "generateInsights";
  }
  return "callModel";
};

export function buildGraph() {
  const workflow = new StateGraph(GraphAnnotation, GraphConfig)
    .addNode("callModel", callModel)
    .addNode("generateInsights", generateInsights)
    .addNode("wasContentGenerated", wasContentGenerated)
    // Always start by checking whether or not to generate insights
    .addConditionalEdges(START, shouldGenerateInsights)
    // Always check if content was generated after calling the model
    .addEdge("callModel", "wasContentGenerated")
    // No further action by the graph is necessary after either
    // generating a response via `callModel`, or rules via `generateInsights`.
    .addEdge("generateInsights", END)
    .addEdge("wasContentGenerated", END);

  return workflow.compile();
}
